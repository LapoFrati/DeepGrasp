{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setGPU: Setting GPU to: 2\n"
     ]
    }
   ],
   "source": [
    "#%env CUDA_VISIBLE_DEVICES=2\n",
    "import setGPU\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch.cuda\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torchvision import models #,transforms, utils,\n",
    "\n",
    "#import math\n",
    "import time\n",
    "import shutil\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "plt.ion()   # interactive mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from /home/frati/new_Grasping/code/dataloaders/GreyColor_dataloader.ipynb\n"
     ]
    }
   ],
   "source": [
    "from JupyterLoader import NotebookFinder\n",
    "import sys\n",
    "sys.meta_path.append(NotebookFinder())\n",
    "from dataloaders.GreyColor_dataloader import ImageNetGenerator\n",
    "\n",
    "data_gen = ImageNetGenerator(data_folder = '/home/frati/Grasping/ImageNet/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from utilities.ipynb\n"
     ]
    }
   ],
   "source": [
    "from utilities import AverageMeter, accuracy, get_trainable_parameters, varify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from models.colorNet import ColorNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColorNet(nn.Module):\n",
    "    def __init__(self, hooks=False, pretrained=None):\n",
    "        super(ColorNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, 1)\n",
    "        self.conv2 = nn.Conv2d(10, 3, 1)\n",
    "        self.conv3 = nn.Conv2d(6, 3, 3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(3,affine=True)\n",
    "        self.bn1 = nn.BatchNorm2d(10,affine=True)\n",
    "\n",
    "        if pretrained is None:\n",
    "            self.init_weights()\n",
    "        else:\n",
    "            state = torch.load(pretrained, map_location=lambda storage, loc: storage)\n",
    "            self.load_state_dict(state['state_dict'])\n",
    "        \n",
    "        def printgradnorm(self, grad_input, grad_output):\n",
    "            print('Inside ' + self.__class__.__name__ + ' backward')\n",
    "            print('{} -> {}'.format(grad_input[0].size(),grad_output[0].size()))\n",
    "            print('grad_in norm: {}'.format(grad_input[0].data.norm()))\n",
    "            print('grad_out norm: {}'.format(grad_output[0].data.norm()))\n",
    "                  \n",
    "        if hooks:\n",
    "            self.conv1.register_backward_hook(printgradnorm)\n",
    "            self.conv2.register_backward_hook(printgradnorm)\n",
    "     \n",
    "    def forward(self, x):\n",
    "        \n",
    "        residual = torch.cat([x,x,x],dim=1)\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(x) \n",
    "        x = torch.cat([x,residual],dim=1)\n",
    "        x = self.conv3(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def init_weights(self):\n",
    "        \"\"\"Initialize the weights.\"\"\"\n",
    "        self.conv1.weight.data.normal_(0, 0.02)\n",
    "        self.conv1.bias.data.fill_(0)\n",
    "        self.conv2.weight.data.normal_(0, 0.02)\n",
    "        self.conv2.bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3, 224, 224])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = ColorNet()\n",
    "fake_batch = Variable(torch.rand(4,1,224,224))\n",
    "t(fake_batch).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distill(data_loaders, model, reference_model, criterion, optimizer, epochs):\n",
    "    since = time.time()\n",
    "    def loss_fn_kd(outputs, labels, teacher_outputs, temperature=3., balance = 0.4):\n",
    "        \"\"\"\n",
    "        Compute the knowledge-distillation (KD) loss given outputs, labels.\n",
    "        \"Hyperparameters\": temperature and alpha\n",
    "        \"\"\"\n",
    "        T = temperature\n",
    "        alpha = balance\n",
    "        KD_loss = nn.KLDivLoss()(F.log_softmax(outputs/T, dim=1),\n",
    "                                 F.softmax(teacher_outputs/T, dim=1)) * (alpha * T * T) + F.cross_entropy(outputs, labels) * (1. - alpha)\n",
    "\n",
    "        return KD_loss\n",
    "    \n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "    num_epochs = epochs\n",
    "    history = []\n",
    "    \n",
    "    best_model_wts = model.state_dict()\n",
    "    best_acc = 0.0\n",
    "    \n",
    "    reference_model.eval()\n",
    "    \n",
    "    dataset_sizes = {key:len(val.dataset) for key,val in data_loaders.items()}\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch+1, num_epochs))\n",
    "        print('-' * 10)\n",
    "        epoch_time = time.time()\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'valid']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "                volatile_inputs = False\n",
    "            else:\n",
    "                model.eval()  # Set model to evaluate mode\n",
    "                volatile_inputs = True\n",
    "                \n",
    "            losses.reset()\n",
    "            top1.reset()\n",
    "            top5.reset()\n",
    "            \n",
    "            batches = len(data_loaders[phase])\n",
    "            # Iterate over data.\n",
    "            for idx,(data) in enumerate(data_loaders[phase]):\n",
    "                # prepare the inputs\n",
    "                color_images, grey_images, labels = data\n",
    "                grey_ims = Variable(grey_images,requires_grad = True).cuda()\n",
    "                color_ims = Variable(color_images,volatile = True).cuda()\n",
    "                targets = Variable(labels,volatile = True).cuda()\n",
    "                \n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad() \n",
    "\n",
    "                # forward\n",
    "                reference_outputs = reference_model(color_ims)\n",
    "                _, reference_preds = torch.max(reference_outputs,1)\n",
    "                \n",
    "                outputs = model(grey_ims)\n",
    "                _,preds = torch.max(outputs,1)\n",
    "                \n",
    "                if phase == 'train':\n",
    "                    #loss = criterion(outputs, reference_preds)\n",
    "                    loss = loss_fn_kd(outputs,targets,reference_outputs)\n",
    "\n",
    "                    # measure accuracy and record loss\n",
    "                    prec1, prec5 = accuracy(outputs, reference_preds, topk=(1, 5))\n",
    "                    losses.update(loss.data[0], targets.size(0))\n",
    "                    top1.update(float(prec1[0]), targets.size(0))\n",
    "                    top5.update(float(prec5[0]), targets.size(0))\n",
    "                else:\n",
    "                    loss = criterion(outputs, targets)\n",
    "                    #loss = loss_fn_kd(outputs,targets,reference_outputs)\n",
    "\n",
    "                    # measure accuracy and record loss\n",
    "                    prec1, prec5 = accuracy(outputs, targets, topk=(1, 5))\n",
    "                    losses.update(loss.data[0], targets.size(0))\n",
    "                    top1.update(float(prec1[0]), targets.size(0))\n",
    "                    top5.update(float(prec5[0]), targets.size(0))\n",
    "                \n",
    "                # backward + optimize only if in training phase\n",
    "                if phase == 'train':\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                else:\n",
    "                    model.eval()\n",
    "                    color = model[0](grey_ims)\n",
    "                    original = data_gen.plot_color(color_ims[0].data.cpu())\n",
    "                    colored = data_gen.plot_color(color[0].data.cpu())\n",
    "                    label = data_gen.translate(labels[0])\n",
    "                    history.append((original,colored,label))\n",
    "                    model.train()\n",
    "                \n",
    "                # progress\n",
    "                print(\"\\r                                                                  \",end=\"\")\n",
    "                #print(\"\\r{}: {}/{} - acc: {:.2f} - loss: {:.2f}\".format(phase,idx,batches,correct/num_tags,loss.data[0]),end=\"\")\n",
    "                print(\"\\r{}: {}/{}\".format(phase,idx,batches),end=\"\")\n",
    "                \n",
    "\n",
    "            print()\n",
    "            print('{}: '\n",
    "              'Loss {loss.avg:.4f}\\t'\n",
    "              'Prec@1 {top1.avg:.3f}\\t'\n",
    "              'Prec@5 {top5.avg:.3f}'.format(phase, loss=losses, top1=top1, top5=top5))\n",
    "            print('Epoch time: {:.3f}'.format(time.time() - epoch_time))\n",
    "            print()\n",
    "            # deep copy the model\n",
    "            epoch_acc = top1.avg\n",
    "            if phase == 'valid' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = model.state_dict()\n",
    "\n",
    "        print()\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format( time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best validation acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model,history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(val_loader, model, criterion, color=False):\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    end = time.time()\n",
    "    for i, (color_images, grey_images, target) in enumerate(val_loader):\n",
    "        target = target.cuda(async=True)\n",
    "        if color == True:\n",
    "            input_var = torch.autograd.Variable(color_images, volatile=True).cuda()\n",
    "        else:\n",
    "            input_var = torch.autograd.Variable(grey_images, volatile=True).cuda()\n",
    "            input_var = torch.cat([input_var,input_var,input_var],dim=1)\n",
    "            \n",
    "        target_var = torch.autograd.Variable(target, volatile=True).cuda()\n",
    "\n",
    "        # compute output\n",
    "        output = model(input_var)\n",
    "        loss = criterion(output, target_var)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1, prec5 = accuracy(output.data, target, topk=(1, 5))\n",
    "        losses.update(loss.data[0], target.size(0))\n",
    "        top1.update(prec1[0], target.size(0))\n",
    "        top5.update(prec5[0], target.size(0))\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        \n",
    "        print('Test: [{0}/{1}]\\t'\n",
    "              'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "              'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "              'Prec@1 {top1.val:.3f} ({top1.avg:.3f})\\t'\n",
    "              'Prec@5 {top5.val:.3f} ({top5.avg:.3f})'.format(\n",
    "               i, len(val_loader), batch_time=batch_time, loss=losses,\n",
    "               top1=top1, top5=top5))\n",
    "\n",
    "    print(' * Prec@1 {top1.avg:.3f} Prec@5 {top5.avg:.3f}'.format(top1=top1, top5=top5))\n",
    "\n",
    "    return top1.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper Parameters\n",
    "num_epochs = 5\n",
    "batch_size = 32\n",
    "#learning_rate = 0.001\n",
    "learning_rate = 0.01\n",
    "best_prec1 = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ColorNet(\n",
       "  (conv1): Conv2d(1, 10, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (conv2): Conv2d(10, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (conv3): Conv2d(6, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn2): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True)\n",
       "  (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "color_net = ColorNet()\n",
    "color_net.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg16 = models.vgg16(pretrained=True)\n",
    "vgg16.cuda()\n",
    "# Freeze model\n",
    "for param in vgg16.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters:\t           0\n",
      "Frozen parameters:\t   138357544\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_trainable_parameters(vgg16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = nn.Sequential(color_net, vgg16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters:\t         244\n",
      "Frozen parameters:\t   138357544\n"
     ]
    }
   ],
   "source": [
    "color_parameters = get_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(color_parameters, lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaders = data_gen.get_loaders(batch_size=batch_size,shuffle=True,num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 1330\n",
      "test 143\n",
      "valid 63\n"
     ]
    }
   ],
   "source": [
    "for phase,loader in loaders.items():\n",
    "    print(phase,len(loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaders['train'] = loaders['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/70\n",
      "----------\n",
      "train: 142/143                                                    \n",
      "train: Loss 2.7024\tPrec@1 21.699\tPrec@5 42.303\n",
      "Epoch time: 37.615\n",
      "\n",
      "valid: 62/63                                                      \n",
      "valid: Loss 4.2391\tPrec@1 22.645\tPrec@5 45.240\n",
      "Epoch time: 49.866\n",
      "\n",
      "\n",
      "Epoch 2/70\n",
      "----------\n",
      "train: 142/143                                                    \n",
      "train: Loss 2.5669\tPrec@1 23.626\tPrec@5 45.763\n",
      "Epoch time: 34.537\n",
      "\n",
      "valid: 62/63                                                      \n",
      "valid: Loss 4.1666\tPrec@1 23.347\tPrec@5 44.088\n",
      "Epoch time: 47.384\n",
      "\n",
      "\n",
      "Epoch 3/70\n",
      "----------\n",
      "train: 142/143                                                    \n",
      "train: Loss 2.5501\tPrec@1 23.757\tPrec@5 46.354\n",
      "Epoch time: 34.528\n",
      "\n",
      "valid: 62/63                                                      \n",
      "valid: Loss 4.1207\tPrec@1 23.848\tPrec@5 45.691\n",
      "Epoch time: 47.095\n",
      "\n",
      "\n",
      "Epoch 4/70\n",
      "----------\n",
      "train: 142/143                                                    \n",
      "train: Loss 2.5353\tPrec@1 24.699\tPrec@5 46.617\n",
      "Epoch time: 34.651\n",
      "\n",
      "valid: 11/63                                                      "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-60:\n",
      "Process Process-57:\n",
      "Process Process-62:\n",
      "Process Process-64:\n",
      "Process Process-61:\n",
      "Process Process-58:\n",
      "Process Process-63:\n",
      "Process Process-59:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/frati/miniconda3/envs/pytorch3.1/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/frati/miniconda3/envs/pytorch3.1/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/frati/miniconda3/envs/pytorch3.1/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/frati/miniconda3/envs/pytorch3.1/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 50, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/home/frati/miniconda3/envs/pytorch3.1/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/frati/miniconda3/envs/pytorch3.1/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/frati/miniconda3/envs/pytorch3.1/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/frati/miniconda3/envs/pytorch3.1/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/frati/miniconda3/envs/pytorch3.1/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/frati/miniconda3/envs/pytorch3.1/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/frati/miniconda3/envs/pytorch3.1/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/frati/miniconda3/envs/pytorch3.1/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 50, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/home/frati/miniconda3/envs/pytorch3.1/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 50, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/home/frati/miniconda3/envs/pytorch3.1/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/frati/miniconda3/envs/pytorch3.1/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/frati/miniconda3/envs/pytorch3.1/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/frati/miniconda3/envs/pytorch3.1/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/frati/miniconda3/envs/pytorch3.1/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/frati/miniconda3/envs/pytorch3.1/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/frati/miniconda3/envs/pytorch3.1/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/frati/miniconda3/envs/pytorch3.1/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 50, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/home/frati/miniconda3/envs/pytorch3.1/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/frati/miniconda3/envs/pytorch3.1/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/frati/miniconda3/envs/pytorch3.1/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 50, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "KeyboardInterrupt\n",
      "  File \"/home/frati/miniconda3/envs/pytorch3.1/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 50, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/home/frati/miniconda3/envs/pytorch3.1/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                  \r",
      "valid: 12/63"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "KeyboardInterrupt\n",
      "  File \"/home/frati/miniconda3/envs/pytorch3.1/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/frati/miniconda3/envs/pytorch3.1/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/frati/miniconda3/envs/pytorch3.1/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 50, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/home/frati/miniconda3/envs/pytorch3.1/lib/python3.6/multiprocessing/queues.py\", line 335, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/home/frati/miniconda3/envs/pytorch3.1/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/frati/miniconda3/envs/pytorch3.1/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/home/frati/miniconda3/envs/pytorch3.1/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/frati/miniconda3/envs/pytorch3.1/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/frati/miniconda3/envs/pytorch3.1/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/home/frati/miniconda3/envs/pytorch3.1/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/frati/miniconda3/envs/pytorch3.1/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/home/frati/miniconda3/envs/pytorch3.1/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 55, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "KeyboardInterrupt\n",
      "  File \"/home/frati/miniconda3/envs/pytorch3.1/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/frati/miniconda3/envs/pytorch3.1/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 55, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/frati/miniconda3/envs/pytorch3.1/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"<string>\", line 25, in __getitem__\n",
      "KeyboardInterrupt\n",
      "  File \"/home/frati/miniconda3/envs/pytorch3.1/lib/python3.6/site-packages/torchvision-0.2.0-py3.6.egg/torchvision/transforms/transforms.py\", line 42, in __call__\n",
      "    img = t(img)\n",
      "  File \"/home/frati/miniconda3/envs/pytorch3.1/lib/python3.6/site-packages/torchvision-0.2.0-py3.6.egg/torchvision/transforms/transforms.py\", line 183, in __call__\n",
      "    return F.center_crop(img, self.size)\n",
      "  File \"/home/frati/miniconda3/envs/pytorch3.1/lib/python3.6/site-packages/torchvision-0.2.0-py3.6.egg/torchvision/transforms/functional.py\", line 265, in center_crop\n",
      "    return crop(img, i, j, th, tw)\n",
      "  File \"/home/frati/miniconda3/envs/pytorch3.1/lib/python3.6/site-packages/torchvision-0.2.0-py3.6.egg/torchvision/transforms/functional.py\", line 255, in crop\n",
      "    return img.crop((j, i, j + w, i + h))\n",
      "  File \"/home/frati/miniconda3/envs/pytorch3.1/lib/python3.6/site-packages/PIL/Image.py\", line 1075, in crop\n",
      "    self.load()\n",
      "  File \"/home/frati/miniconda3/envs/pytorch3.1/lib/python3.6/site-packages/PIL/ImageFile.py\", line 236, in load\n",
      "    n, err_code = decoder.decode(b)\n",
      "KeyboardInterrupt\n",
      "Exception ignored in: <bound method DataLoaderIter.__del__ of <torch.utils.data.dataloader.DataLoaderIter object at 0x7f6a538ce3c8>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/frati/miniconda3/envs/pytorch3.1/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 333, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/frati/miniconda3/envs/pytorch3.1/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 319, in _shutdown_workers\n",
      "    self.data_queue.get()\n",
      "  File \"/home/frati/miniconda3/envs/pytorch3.1/lib/python3.6/multiprocessing/queues.py\", line 337, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/home/frati/miniconda3/envs/pytorch3.1/lib/python3.6/site-packages/torch/multiprocessing/reductions.py\", line 70, in rebuild_storage_fd\n",
      "    fd = df.detach()\n",
      "  File \"/home/frati/miniconda3/envs/pytorch3.1/lib/python3.6/multiprocessing/resource_sharer.py\", line 57, in detach\n",
      "    with _resource_sharer.get_connection(self._id) as conn:\n",
      "  File \"/home/frati/miniconda3/envs/pytorch3.1/lib/python3.6/multiprocessing/resource_sharer.py\", line 87, in get_connection\n",
      "    c = Client(address, authkey=process.current_process().authkey)\n",
      "  File \"/home/frati/miniconda3/envs/pytorch3.1/lib/python3.6/multiprocessing/connection.py\", line 487, in Client\n",
      "    c = SocketClient(address)\n",
      "  File \"/home/frati/miniconda3/envs/pytorch3.1/lib/python3.6/multiprocessing/connection.py\", line 614, in SocketClient\n",
      "    s.connect(address)\n",
      "FileNotFoundError: [Errno 2] No such file or directory\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-49a9d487b6c5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdistill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_loaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreference_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvgg16\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m70\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-3b5229200e6f>\u001b[0m in \u001b[0;36mdistill\u001b[0;34m(data_loaders, model, reference_model, criterion, optimizer, epochs)\u001b[0m\n\u001b[1;32m     77\u001b[0m                     \u001b[0;31m# measure accuracy and record loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m                     \u001b[0mprec1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprec5\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m                     \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m                     \u001b[0mtop1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprec1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m                     \u001b[0mtop5\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprec5\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model, history = distill(data_loaders=loaders, model=model, reference_model=vgg16,criterion=criterion, optimizer=optimizer, epochs=70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: [0/143]\tTime 1.018 (1.018)\tLoss 1.4996 (1.4996)\tPrec@1 59.375 (59.375)\tPrec@5 87.500 (87.500)\n",
      "Test: [1/143]\tTime 0.082 (0.550)\tLoss 0.6304 (1.0650)\tPrec@1 84.375 (71.875)\tPrec@5 96.875 (92.188)\n",
      "Test: [2/143]\tTime 0.091 (0.397)\tLoss 0.8773 (1.0024)\tPrec@1 78.125 (73.958)\tPrec@5 90.625 (91.667)\n",
      "Test: [3/143]\tTime 0.085 (0.319)\tLoss 1.1305 (1.0345)\tPrec@1 87.500 (77.344)\tPrec@5 87.500 (90.625)\n",
      "Test: [4/143]\tTime 0.094 (0.274)\tLoss 1.6643 (1.1604)\tPrec@1 62.500 (74.375)\tPrec@5 81.250 (88.750)\n",
      "Test: [5/143]\tTime 0.084 (0.242)\tLoss 1.1573 (1.1599)\tPrec@1 68.750 (73.438)\tPrec@5 90.625 (89.062)\n",
      "Test: [6/143]\tTime 0.083 (0.219)\tLoss 1.4287 (1.1983)\tPrec@1 71.875 (73.214)\tPrec@5 90.625 (89.286)\n",
      "Test: [7/143]\tTime 0.085 (0.202)\tLoss 0.8921 (1.1600)\tPrec@1 78.125 (73.828)\tPrec@5 96.875 (90.234)\n",
      "Test: [8/143]\tTime 0.082 (0.189)\tLoss 1.9480 (1.2476)\tPrec@1 56.250 (71.875)\tPrec@5 87.500 (89.931)\n",
      "Test: [9/143]\tTime 0.081 (0.178)\tLoss 1.0805 (1.2309)\tPrec@1 62.500 (70.938)\tPrec@5 93.750 (90.312)\n",
      "Test: [10/143]\tTime 0.081 (0.169)\tLoss 1.0369 (1.2133)\tPrec@1 68.750 (70.739)\tPrec@5 96.875 (90.909)\n",
      "Test: [11/143]\tTime 0.079 (0.162)\tLoss 1.6148 (1.2467)\tPrec@1 59.375 (69.792)\tPrec@5 78.125 (89.844)\n",
      "Test: [12/143]\tTime 0.080 (0.156)\tLoss 1.5430 (1.2695)\tPrec@1 62.500 (69.231)\tPrec@5 90.625 (89.904)\n",
      "Test: [13/143]\tTime 0.080 (0.150)\tLoss 1.0996 (1.2574)\tPrec@1 59.375 (68.527)\tPrec@5 90.625 (89.955)\n",
      "Test: [14/143]\tTime 0.090 (0.146)\tLoss 1.1480 (1.2501)\tPrec@1 68.750 (68.542)\tPrec@5 90.625 (90.000)\n",
      "Test: [15/143]\tTime 0.080 (0.142)\tLoss 1.1545 (1.2441)\tPrec@1 62.500 (68.164)\tPrec@5 90.625 (90.039)\n",
      "Test: [16/143]\tTime 0.080 (0.138)\tLoss 0.6675 (1.2102)\tPrec@1 84.375 (69.118)\tPrec@5 100.000 (90.625)\n",
      "Test: [17/143]\tTime 0.076 (0.135)\tLoss 1.4256 (1.2221)\tPrec@1 68.750 (69.097)\tPrec@5 84.375 (90.278)\n",
      "Test: [18/143]\tTime 0.080 (0.132)\tLoss 1.3045 (1.2265)\tPrec@1 65.625 (68.914)\tPrec@5 87.500 (90.132)\n",
      "Test: [19/143]\tTime 0.080 (0.129)\tLoss 0.6005 (1.1952)\tPrec@1 84.375 (69.688)\tPrec@5 100.000 (90.625)\n",
      "Test: [20/143]\tTime 0.078 (0.127)\tLoss 0.9919 (1.1855)\tPrec@1 78.125 (70.089)\tPrec@5 90.625 (90.625)\n",
      "Test: [21/143]\tTime 0.079 (0.125)\tLoss 1.3353 (1.1923)\tPrec@1 68.750 (70.028)\tPrec@5 87.500 (90.483)\n",
      "Test: [22/143]\tTime 0.080 (0.123)\tLoss 2.3905 (1.2444)\tPrec@1 50.000 (69.158)\tPrec@5 68.750 (89.538)\n",
      "Test: [23/143]\tTime 0.083 (0.121)\tLoss 1.0825 (1.2377)\tPrec@1 71.875 (69.271)\tPrec@5 90.625 (89.583)\n",
      "Test: [24/143]\tTime 0.088 (0.120)\tLoss 1.2157 (1.2368)\tPrec@1 75.000 (69.500)\tPrec@5 87.500 (89.500)\n",
      "Test: [25/143]\tTime 0.088 (0.119)\tLoss 1.1480 (1.2334)\tPrec@1 71.875 (69.591)\tPrec@5 90.625 (89.543)\n",
      "Test: [26/143]\tTime 0.079 (0.117)\tLoss 1.6272 (1.2480)\tPrec@1 56.250 (69.097)\tPrec@5 87.500 (89.468)\n",
      "Test: [27/143]\tTime 0.079 (0.116)\tLoss 1.4316 (1.2545)\tPrec@1 59.375 (68.750)\tPrec@5 81.250 (89.174)\n",
      "Test: [28/143]\tTime 0.078 (0.114)\tLoss 1.2681 (1.2550)\tPrec@1 75.000 (68.966)\tPrec@5 87.500 (89.116)\n",
      "Test: [29/143]\tTime 0.078 (0.113)\tLoss 0.6784 (1.2358)\tPrec@1 78.125 (69.271)\tPrec@5 96.875 (89.375)\n",
      "Test: [30/143]\tTime 0.082 (0.112)\tLoss 1.2597 (1.2365)\tPrec@1 75.000 (69.456)\tPrec@5 84.375 (89.214)\n",
      "Test: [31/143]\tTime 0.078 (0.111)\tLoss 1.1866 (1.2350)\tPrec@1 65.625 (69.336)\tPrec@5 90.625 (89.258)\n",
      "Test: [32/143]\tTime 0.076 (0.110)\tLoss 1.5397 (1.2442)\tPrec@1 68.750 (69.318)\tPrec@5 87.500 (89.205)\n",
      "Test: [33/143]\tTime 0.082 (0.109)\tLoss 0.9610 (1.2359)\tPrec@1 78.125 (69.577)\tPrec@5 96.875 (89.430)\n",
      "Test: [34/143]\tTime 0.087 (0.109)\tLoss 1.3512 (1.2392)\tPrec@1 62.500 (69.375)\tPrec@5 87.500 (89.375)\n",
      "Test: [35/143]\tTime 0.077 (0.108)\tLoss 1.2845 (1.2404)\tPrec@1 65.625 (69.271)\tPrec@5 93.750 (89.497)\n",
      "Test: [36/143]\tTime 0.087 (0.107)\tLoss 1.9555 (1.2598)\tPrec@1 56.250 (68.919)\tPrec@5 78.125 (89.189)\n",
      "Test: [37/143]\tTime 0.078 (0.106)\tLoss 1.2063 (1.2584)\tPrec@1 78.125 (69.161)\tPrec@5 84.375 (89.062)\n",
      "Test: [38/143]\tTime 0.077 (0.106)\tLoss 1.0999 (1.2543)\tPrec@1 68.750 (69.151)\tPrec@5 90.625 (89.103)\n",
      "Test: [39/143]\tTime 0.077 (0.105)\tLoss 1.9849 (1.2726)\tPrec@1 43.750 (68.516)\tPrec@5 84.375 (88.984)\n",
      "Test: [40/143]\tTime 0.078 (0.104)\tLoss 1.2079 (1.2710)\tPrec@1 62.500 (68.369)\tPrec@5 84.375 (88.872)\n",
      "Test: [41/143]\tTime 0.077 (0.104)\tLoss 1.2828 (1.2713)\tPrec@1 65.625 (68.304)\tPrec@5 93.750 (88.988)\n",
      "Test: [42/143]\tTime 0.077 (0.103)\tLoss 1.4182 (1.2747)\tPrec@1 62.500 (68.169)\tPrec@5 87.500 (88.953)\n",
      "Test: [43/143]\tTime 0.078 (0.102)\tLoss 1.4473 (1.2786)\tPrec@1 62.500 (68.040)\tPrec@5 78.125 (88.707)\n",
      "Test: [44/143]\tTime 0.090 (0.102)\tLoss 1.4271 (1.2819)\tPrec@1 65.625 (67.986)\tPrec@5 81.250 (88.542)\n",
      "Test: [45/143]\tTime 0.088 (0.102)\tLoss 1.7232 (1.2915)\tPrec@1 62.500 (67.867)\tPrec@5 75.000 (88.247)\n",
      "Test: [46/143]\tTime 0.078 (0.101)\tLoss 1.2247 (1.2901)\tPrec@1 71.875 (67.952)\tPrec@5 87.500 (88.231)\n",
      "Test: [47/143]\tTime 0.077 (0.101)\tLoss 1.2426 (1.2891)\tPrec@1 56.250 (67.708)\tPrec@5 90.625 (88.281)\n",
      "Test: [48/143]\tTime 0.077 (0.100)\tLoss 1.1703 (1.2867)\tPrec@1 65.625 (67.666)\tPrec@5 90.625 (88.329)\n",
      "Test: [49/143]\tTime 0.078 (0.100)\tLoss 1.2506 (1.2859)\tPrec@1 71.875 (67.750)\tPrec@5 87.500 (88.312)\n",
      "Test: [50/143]\tTime 0.078 (0.100)\tLoss 1.1025 (1.2823)\tPrec@1 68.750 (67.770)\tPrec@5 90.625 (88.358)\n",
      "Test: [51/143]\tTime 0.080 (0.099)\tLoss 1.4591 (1.2857)\tPrec@1 59.375 (67.608)\tPrec@5 87.500 (88.341)\n",
      "Test: [52/143]\tTime 0.080 (0.099)\tLoss 0.9992 (1.2803)\tPrec@1 71.875 (67.689)\tPrec@5 96.875 (88.502)\n",
      "Test: [53/143]\tTime 0.084 (0.099)\tLoss 1.7537 (1.2891)\tPrec@1 59.375 (67.535)\tPrec@5 87.500 (88.484)\n",
      "Test: [54/143]\tTime 0.080 (0.098)\tLoss 1.0360 (1.2845)\tPrec@1 71.875 (67.614)\tPrec@5 93.750 (88.580)\n",
      "Test: [55/143]\tTime 0.080 (0.098)\tLoss 0.6975 (1.2740)\tPrec@1 81.250 (67.857)\tPrec@5 96.875 (88.728)\n",
      "Test: [56/143]\tTime 0.080 (0.098)\tLoss 1.1488 (1.2718)\tPrec@1 65.625 (67.818)\tPrec@5 87.500 (88.706)\n",
      "Test: [57/143]\tTime 0.079 (0.097)\tLoss 0.9192 (1.2657)\tPrec@1 81.250 (68.050)\tPrec@5 90.625 (88.739)\n",
      "Test: [58/143]\tTime 0.081 (0.097)\tLoss 2.0209 (1.2785)\tPrec@1 56.250 (67.850)\tPrec@5 78.125 (88.559)\n",
      "Test: [59/143]\tTime 0.079 (0.097)\tLoss 1.2023 (1.2773)\tPrec@1 68.750 (67.865)\tPrec@5 84.375 (88.490)\n",
      "Test: [60/143]\tTime 0.085 (0.096)\tLoss 1.2838 (1.2774)\tPrec@1 75.000 (67.982)\tPrec@5 87.500 (88.473)\n",
      "Test: [61/143]\tTime 0.080 (0.096)\tLoss 1.3701 (1.2789)\tPrec@1 71.875 (68.044)\tPrec@5 84.375 (88.407)\n",
      "Test: [62/143]\tTime 0.079 (0.096)\tLoss 1.5411 (1.2830)\tPrec@1 71.875 (68.105)\tPrec@5 90.625 (88.442)\n",
      "Test: [63/143]\tTime 0.080 (0.096)\tLoss 0.8339 (1.2760)\tPrec@1 81.250 (68.311)\tPrec@5 96.875 (88.574)\n",
      "Test: [64/143]\tTime 0.079 (0.095)\tLoss 1.0268 (1.2722)\tPrec@1 75.000 (68.413)\tPrec@5 90.625 (88.606)\n",
      "Test: [65/143]\tTime 0.099 (0.095)\tLoss 1.4500 (1.2749)\tPrec@1 65.625 (68.371)\tPrec@5 81.250 (88.494)\n",
      "Test: [66/143]\tTime 0.079 (0.095)\tLoss 1.3116 (1.2754)\tPrec@1 71.875 (68.424)\tPrec@5 84.375 (88.433)\n",
      "Test: [67/143]\tTime 0.085 (0.095)\tLoss 1.8084 (1.2833)\tPrec@1 62.500 (68.336)\tPrec@5 81.250 (88.327)\n",
      "Test: [68/143]\tTime 0.078 (0.095)\tLoss 1.1735 (1.2817)\tPrec@1 62.500 (68.252)\tPrec@5 93.750 (88.406)\n",
      "Test: [69/143]\tTime 0.080 (0.095)\tLoss 1.0955 (1.2790)\tPrec@1 81.250 (68.438)\tPrec@5 87.500 (88.393)\n",
      "Test: [70/143]\tTime 0.081 (0.094)\tLoss 1.3637 (1.2802)\tPrec@1 65.625 (68.398)\tPrec@5 87.500 (88.380)\n",
      "Test: [71/143]\tTime 0.088 (0.094)\tLoss 0.8128 (1.2737)\tPrec@1 81.250 (68.576)\tPrec@5 96.875 (88.498)\n",
      "Test: [72/143]\tTime 0.080 (0.094)\tLoss 1.0274 (1.2703)\tPrec@1 68.750 (68.579)\tPrec@5 93.750 (88.570)\n",
      "Test: [73/143]\tTime 0.081 (0.094)\tLoss 1.0252 (1.2670)\tPrec@1 81.250 (68.750)\tPrec@5 87.500 (88.556)\n",
      "Test: [74/143]\tTime 0.099 (0.094)\tLoss 1.6953 (1.2727)\tPrec@1 59.375 (68.625)\tPrec@5 84.375 (88.500)\n",
      "Test: [75/143]\tTime 0.082 (0.094)\tLoss 1.3008 (1.2731)\tPrec@1 68.750 (68.627)\tPrec@5 90.625 (88.528)\n",
      "Test: [76/143]\tTime 0.080 (0.094)\tLoss 1.1017 (1.2709)\tPrec@1 62.500 (68.547)\tPrec@5 93.750 (88.596)\n",
      "Test: [77/143]\tTime 0.086 (0.094)\tLoss 1.2798 (1.2710)\tPrec@1 68.750 (68.550)\tPrec@5 96.875 (88.702)\n",
      "Test: [78/143]\tTime 0.079 (0.093)\tLoss 1.5593 (1.2746)\tPrec@1 59.375 (68.434)\tPrec@5 84.375 (88.647)\n",
      "Test: [79/143]\tTime 0.091 (0.093)\tLoss 1.2849 (1.2748)\tPrec@1 78.125 (68.555)\tPrec@5 87.500 (88.633)\n",
      "Test: [80/143]\tTime 0.078 (0.093)\tLoss 1.2431 (1.2744)\tPrec@1 75.000 (68.634)\tPrec@5 90.625 (88.657)\n",
      "Test: [81/143]\tTime 0.079 (0.093)\tLoss 1.0448 (1.2716)\tPrec@1 71.875 (68.674)\tPrec@5 90.625 (88.681)\n",
      "Test: [82/143]\tTime 0.079 (0.093)\tLoss 1.1327 (1.2699)\tPrec@1 78.125 (68.788)\tPrec@5 93.750 (88.742)\n",
      "Test: [83/143]\tTime 0.078 (0.093)\tLoss 1.5311 (1.2730)\tPrec@1 62.500 (68.713)\tPrec@5 84.375 (88.690)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: [84/143]\tTime 0.077 (0.092)\tLoss 2.0212 (1.2818)\tPrec@1 53.125 (68.529)\tPrec@5 81.250 (88.603)\n",
      "Test: [85/143]\tTime 0.078 (0.092)\tLoss 1.2959 (1.2820)\tPrec@1 71.875 (68.568)\tPrec@5 87.500 (88.590)\n",
      "Test: [86/143]\tTime 0.080 (0.092)\tLoss 1.1333 (1.2803)\tPrec@1 78.125 (68.678)\tPrec@5 84.375 (88.542)\n",
      "Test: [87/143]\tTime 0.080 (0.092)\tLoss 1.7885 (1.2860)\tPrec@1 56.250 (68.537)\tPrec@5 87.500 (88.530)\n",
      "Test: [88/143]\tTime 0.083 (0.092)\tLoss 2.0240 (1.2943)\tPrec@1 62.500 (68.469)\tPrec@5 84.375 (88.483)\n",
      "Test: [89/143]\tTime 0.089 (0.092)\tLoss 1.0242 (1.2913)\tPrec@1 62.500 (68.403)\tPrec@5 96.875 (88.576)\n",
      "Test: [90/143]\tTime 0.085 (0.092)\tLoss 2.0016 (1.2991)\tPrec@1 43.750 (68.132)\tPrec@5 81.250 (88.496)\n",
      "Test: [91/143]\tTime 0.102 (0.092)\tLoss 1.4286 (1.3006)\tPrec@1 53.125 (67.969)\tPrec@5 87.500 (88.485)\n",
      "Test: [92/143]\tTime 0.077 (0.092)\tLoss 1.7996 (1.3059)\tPrec@1 71.875 (68.011)\tPrec@5 81.250 (88.407)\n",
      "Test: [93/143]\tTime 0.080 (0.092)\tLoss 1.2670 (1.3055)\tPrec@1 65.625 (67.985)\tPrec@5 87.500 (88.398)\n",
      "Test: [94/143]\tTime 0.081 (0.092)\tLoss 1.7475 (1.3102)\tPrec@1 71.875 (68.026)\tPrec@5 78.125 (88.289)\n",
      "Test: [95/143]\tTime 0.077 (0.091)\tLoss 1.7305 (1.3145)\tPrec@1 56.250 (67.904)\tPrec@5 84.375 (88.249)\n",
      "Test: [96/143]\tTime 0.089 (0.091)\tLoss 1.0424 (1.3117)\tPrec@1 59.375 (67.816)\tPrec@5 93.750 (88.305)\n",
      "Test: [97/143]\tTime 0.081 (0.091)\tLoss 1.5781 (1.3144)\tPrec@1 59.375 (67.730)\tPrec@5 87.500 (88.297)\n",
      "Test: [98/143]\tTime 0.090 (0.091)\tLoss 1.4605 (1.3159)\tPrec@1 53.125 (67.582)\tPrec@5 90.625 (88.321)\n",
      "Test: [99/143]\tTime 0.079 (0.091)\tLoss 1.6874 (1.3196)\tPrec@1 62.500 (67.531)\tPrec@5 81.250 (88.250)\n",
      "Test: [100/143]\tTime 0.080 (0.091)\tLoss 1.2906 (1.3193)\tPrec@1 65.625 (67.512)\tPrec@5 84.375 (88.212)\n",
      "Test: [101/143]\tTime 0.095 (0.091)\tLoss 1.4521 (1.3207)\tPrec@1 62.500 (67.463)\tPrec@5 81.250 (88.143)\n",
      "Test: [102/143]\tTime 0.080 (0.091)\tLoss 1.6761 (1.3241)\tPrec@1 62.500 (67.415)\tPrec@5 78.125 (88.046)\n",
      "Test: [103/143]\tTime 0.079 (0.091)\tLoss 1.0215 (1.3212)\tPrec@1 78.125 (67.518)\tPrec@5 87.500 (88.041)\n",
      "Test: [104/143]\tTime 0.077 (0.091)\tLoss 1.3114 (1.3211)\tPrec@1 56.250 (67.411)\tPrec@5 90.625 (88.065)\n",
      "Test: [105/143]\tTime 0.082 (0.091)\tLoss 2.0416 (1.3279)\tPrec@1 65.625 (67.394)\tPrec@5 78.125 (87.972)\n",
      "Test: [106/143]\tTime 0.093 (0.091)\tLoss 1.7212 (1.3316)\tPrec@1 65.625 (67.377)\tPrec@5 84.375 (87.938)\n",
      "Test: [107/143]\tTime 0.089 (0.091)\tLoss 0.9409 (1.3280)\tPrec@1 84.375 (67.535)\tPrec@5 96.875 (88.021)\n",
      "Test: [108/143]\tTime 0.079 (0.090)\tLoss 1.2021 (1.3268)\tPrec@1 65.625 (67.517)\tPrec@5 90.625 (88.045)\n",
      "Test: [109/143]\tTime 0.081 (0.090)\tLoss 1.1739 (1.3254)\tPrec@1 71.875 (67.557)\tPrec@5 90.625 (88.068)\n",
      "Test: [110/143]\tTime 0.079 (0.090)\tLoss 0.8741 (1.3213)\tPrec@1 75.000 (67.624)\tPrec@5 90.625 (88.091)\n",
      "Test: [111/143]\tTime 0.079 (0.090)\tLoss 0.8970 (1.3176)\tPrec@1 81.250 (67.746)\tPrec@5 93.750 (88.142)\n",
      "Test: [112/143]\tTime 0.079 (0.090)\tLoss 1.8509 (1.3223)\tPrec@1 62.500 (67.699)\tPrec@5 87.500 (88.136)\n",
      "Test: [113/143]\tTime 0.089 (0.090)\tLoss 1.0611 (1.3200)\tPrec@1 71.875 (67.736)\tPrec@5 93.750 (88.185)\n",
      "Test: [114/143]\tTime 0.079 (0.090)\tLoss 1.2253 (1.3192)\tPrec@1 75.000 (67.799)\tPrec@5 90.625 (88.207)\n",
      "Test: [115/143]\tTime 0.079 (0.090)\tLoss 0.9487 (1.3160)\tPrec@1 84.375 (67.942)\tPrec@5 87.500 (88.200)\n",
      "Test: [116/143]\tTime 0.082 (0.090)\tLoss 1.1197 (1.3143)\tPrec@1 75.000 (68.002)\tPrec@5 87.500 (88.194)\n",
      "Test: [117/143]\tTime 0.104 (0.090)\tLoss 1.1469 (1.3129)\tPrec@1 59.375 (67.929)\tPrec@5 90.625 (88.215)\n",
      "Test: [118/143]\tTime 0.079 (0.090)\tLoss 1.3765 (1.3134)\tPrec@1 65.625 (67.910)\tPrec@5 87.500 (88.209)\n",
      "Test: [119/143]\tTime 0.080 (0.090)\tLoss 1.0683 (1.3114)\tPrec@1 78.125 (67.995)\tPrec@5 90.625 (88.229)\n",
      "Test: [120/143]\tTime 0.079 (0.090)\tLoss 1.5441 (1.3133)\tPrec@1 59.375 (67.924)\tPrec@5 81.250 (88.171)\n",
      "Test: [121/143]\tTime 0.079 (0.090)\tLoss 1.2308 (1.3126)\tPrec@1 62.500 (67.879)\tPrec@5 90.625 (88.192)\n",
      "Test: [122/143]\tTime 0.079 (0.090)\tLoss 1.5211 (1.3143)\tPrec@1 65.625 (67.861)\tPrec@5 90.625 (88.211)\n",
      "Test: [123/143]\tTime 0.095 (0.090)\tLoss 1.0348 (1.3121)\tPrec@1 62.500 (67.818)\tPrec@5 93.750 (88.256)\n",
      "Test: [124/143]\tTime 0.080 (0.089)\tLoss 1.1912 (1.3111)\tPrec@1 71.875 (67.850)\tPrec@5 87.500 (88.250)\n",
      "Test: [125/143]\tTime 0.079 (0.089)\tLoss 1.1263 (1.3096)\tPrec@1 68.750 (67.857)\tPrec@5 87.500 (88.244)\n",
      "Test: [126/143]\tTime 0.079 (0.089)\tLoss 1.6607 (1.3124)\tPrec@1 62.500 (67.815)\tPrec@5 84.375 (88.214)\n",
      "Test: [127/143]\tTime 0.081 (0.089)\tLoss 1.4789 (1.3137)\tPrec@1 65.625 (67.798)\tPrec@5 81.250 (88.159)\n",
      "Test: [128/143]\tTime 0.078 (0.089)\tLoss 1.1023 (1.3120)\tPrec@1 71.875 (67.829)\tPrec@5 90.625 (88.178)\n",
      "Test: [129/143]\tTime 0.078 (0.089)\tLoss 1.3066 (1.3120)\tPrec@1 62.500 (67.788)\tPrec@5 87.500 (88.173)\n",
      "Test: [130/143]\tTime 0.084 (0.089)\tLoss 1.1216 (1.3106)\tPrec@1 68.750 (67.796)\tPrec@5 90.625 (88.192)\n",
      "Test: [131/143]\tTime 0.080 (0.089)\tLoss 1.6657 (1.3132)\tPrec@1 59.375 (67.732)\tPrec@5 87.500 (88.187)\n",
      "Test: [132/143]\tTime 0.080 (0.089)\tLoss 1.0638 (1.3114)\tPrec@1 68.750 (67.740)\tPrec@5 93.750 (88.228)\n",
      "Test: [133/143]\tTime 0.077 (0.089)\tLoss 0.7758 (1.3074)\tPrec@1 75.000 (67.794)\tPrec@5 100.000 (88.316)\n",
      "Test: [134/143]\tTime 0.078 (0.089)\tLoss 1.1857 (1.3065)\tPrec@1 68.750 (67.801)\tPrec@5 93.750 (88.356)\n",
      "Test: [135/143]\tTime 0.078 (0.089)\tLoss 1.6937 (1.3093)\tPrec@1 68.750 (67.808)\tPrec@5 90.625 (88.373)\n",
      "Test: [136/143]\tTime 0.076 (0.089)\tLoss 0.9525 (1.3067)\tPrec@1 78.125 (67.883)\tPrec@5 96.875 (88.435)\n",
      "Test: [137/143]\tTime 0.075 (0.088)\tLoss 1.0091 (1.3046)\tPrec@1 71.875 (67.912)\tPrec@5 93.750 (88.474)\n",
      "Test: [138/143]\tTime 0.075 (0.088)\tLoss 0.9713 (1.3022)\tPrec@1 78.125 (67.986)\tPrec@5 87.500 (88.467)\n",
      "Test: [139/143]\tTime 0.077 (0.088)\tLoss 0.8060 (1.2986)\tPrec@1 71.875 (68.013)\tPrec@5 96.875 (88.527)\n",
      "Test: [140/143]\tTime 0.082 (0.088)\tLoss 1.9754 (1.3034)\tPrec@1 65.625 (67.996)\tPrec@5 81.250 (88.475)\n",
      "Test: [141/143]\tTime 0.088 (0.088)\tLoss 1.0240 (1.3014)\tPrec@1 68.750 (68.002)\tPrec@5 93.750 (88.512)\n",
      "Test: [142/143]\tTime 0.064 (0.088)\tLoss 1.8251 (1.3041)\tPrec@1 65.217 (67.988)\tPrec@5 78.261 (88.461)\n",
      " * Prec@1 67.988 Prec@5 88.461\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "67.98773812464611"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(loaders['test'],vgg16,criterion,color=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: [0/143]\tTime 0.820 (0.820)\tLoss 1.9504 (1.9504)\tPrec@1 53.125 (53.125)\tPrec@5 78.125 (78.125)\n",
      "Test: [1/143]\tTime 0.077 (0.448)\tLoss 1.9913 (1.9709)\tPrec@1 65.625 (59.375)\tPrec@5 78.125 (78.125)\n",
      "Test: [2/143]\tTime 0.096 (0.331)\tLoss 1.5757 (1.8392)\tPrec@1 53.125 (57.292)\tPrec@5 93.750 (83.333)\n",
      "Test: [3/143]\tTime 0.085 (0.269)\tLoss 2.1791 (1.9242)\tPrec@1 37.500 (52.344)\tPrec@5 84.375 (83.594)\n",
      "Test: [4/143]\tTime 0.091 (0.234)\tLoss 2.8618 (2.1117)\tPrec@1 43.750 (50.625)\tPrec@5 65.625 (80.000)\n",
      "Test: [5/143]\tTime 0.083 (0.209)\tLoss 1.9720 (2.0884)\tPrec@1 62.500 (52.604)\tPrec@5 75.000 (79.167)\n",
      "Test: [6/143]\tTime 0.090 (0.192)\tLoss 1.8885 (2.0598)\tPrec@1 56.250 (53.125)\tPrec@5 81.250 (79.464)\n",
      "Test: [7/143]\tTime 0.078 (0.177)\tLoss 1.4413 (1.9825)\tPrec@1 75.000 (55.859)\tPrec@5 84.375 (80.078)\n",
      "Test: [8/143]\tTime 0.088 (0.168)\tLoss 2.0481 (1.9898)\tPrec@1 53.125 (55.556)\tPrec@5 75.000 (79.514)\n",
      "Test: [9/143]\tTime 0.076 (0.158)\tLoss 2.3105 (2.0219)\tPrec@1 43.750 (54.375)\tPrec@5 71.875 (78.750)\n",
      "Test: [10/143]\tTime 0.077 (0.151)\tLoss 1.9651 (2.0167)\tPrec@1 50.000 (53.977)\tPrec@5 71.875 (78.125)\n",
      "Test: [11/143]\tTime 0.075 (0.145)\tLoss 2.2016 (2.0321)\tPrec@1 43.750 (53.125)\tPrec@5 78.125 (78.125)\n",
      "Test: [12/143]\tTime 0.086 (0.140)\tLoss 1.7788 (2.0126)\tPrec@1 56.250 (53.365)\tPrec@5 84.375 (78.606)\n",
      "Test: [13/143]\tTime 0.078 (0.136)\tLoss 1.6525 (1.9869)\tPrec@1 62.500 (54.018)\tPrec@5 84.375 (79.018)\n",
      "Test: [14/143]\tTime 0.085 (0.132)\tLoss 2.1535 (1.9980)\tPrec@1 50.000 (53.750)\tPrec@5 75.000 (78.750)\n",
      "Test: [15/143]\tTime 0.088 (0.130)\tLoss 2.5461 (2.0323)\tPrec@1 43.750 (53.125)\tPrec@5 75.000 (78.516)\n",
      "Test: [16/143]\tTime 0.074 (0.126)\tLoss 0.9521 (1.9687)\tPrec@1 65.625 (53.860)\tPrec@5 93.750 (79.412)\n",
      "Test: [17/143]\tTime 0.094 (0.124)\tLoss 2.1710 (1.9800)\tPrec@1 43.750 (53.299)\tPrec@5 78.125 (79.340)\n",
      "Test: [18/143]\tTime 0.096 (0.123)\tLoss 2.2993 (1.9968)\tPrec@1 56.250 (53.454)\tPrec@5 71.875 (78.947)\n",
      "Test: [19/143]\tTime 0.091 (0.121)\tLoss 2.0196 (1.9979)\tPrec@1 59.375 (53.750)\tPrec@5 71.875 (78.594)\n",
      "Test: [20/143]\tTime 0.074 (0.119)\tLoss 1.6902 (1.9833)\tPrec@1 56.250 (53.869)\tPrec@5 87.500 (79.018)\n",
      "Test: [21/143]\tTime 0.079 (0.117)\tLoss 1.8552 (1.9774)\tPrec@1 56.250 (53.977)\tPrec@5 75.000 (78.835)\n",
      "Test: [22/143]\tTime 0.089 (0.116)\tLoss 1.8893 (1.9736)\tPrec@1 50.000 (53.804)\tPrec@5 81.250 (78.940)\n",
      "Test: [23/143]\tTime 0.076 (0.114)\tLoss 2.5303 (1.9968)\tPrec@1 46.875 (53.516)\tPrec@5 68.750 (78.516)\n",
      "Test: [24/143]\tTime 0.076 (0.113)\tLoss 2.2261 (2.0060)\tPrec@1 50.000 (53.375)\tPrec@5 78.125 (78.500)\n",
      "Test: [25/143]\tTime 0.095 (0.112)\tLoss 2.1198 (2.0103)\tPrec@1 65.625 (53.846)\tPrec@5 81.250 (78.606)\n",
      "Test: [26/143]\tTime 0.073 (0.111)\tLoss 2.2323 (2.0186)\tPrec@1 56.250 (53.935)\tPrec@5 78.125 (78.588)\n",
      "Test: [27/143]\tTime 0.076 (0.109)\tLoss 1.8549 (2.0127)\tPrec@1 56.250 (54.018)\tPrec@5 78.125 (78.571)\n",
      "Test: [28/143]\tTime 0.087 (0.109)\tLoss 2.6384 (2.0343)\tPrec@1 40.625 (53.556)\tPrec@5 62.500 (78.017)\n",
      "Test: [29/143]\tTime 0.088 (0.108)\tLoss 2.2739 (2.0423)\tPrec@1 46.875 (53.333)\tPrec@5 71.875 (77.812)\n",
      "Test: [30/143]\tTime 0.074 (0.107)\tLoss 1.9471 (2.0392)\tPrec@1 56.250 (53.427)\tPrec@5 78.125 (77.823)\n",
      "Test: [31/143]\tTime 0.077 (0.106)\tLoss 2.3237 (2.0481)\tPrec@1 43.750 (53.125)\tPrec@5 62.500 (77.344)\n",
      "Test: [32/143]\tTime 0.081 (0.105)\tLoss 1.9804 (2.0460)\tPrec@1 53.125 (53.125)\tPrec@5 84.375 (77.557)\n",
      "Test: [33/143]\tTime 0.080 (0.104)\tLoss 2.3219 (2.0542)\tPrec@1 50.000 (53.033)\tPrec@5 65.625 (77.206)\n",
      "Test: [34/143]\tTime 0.074 (0.104)\tLoss 1.5684 (2.0403)\tPrec@1 68.750 (53.482)\tPrec@5 81.250 (77.321)\n",
      "Test: [35/143]\tTime 0.079 (0.103)\tLoss 2.1547 (2.0435)\tPrec@1 43.750 (53.212)\tPrec@5 78.125 (77.344)\n",
      "Test: [36/143]\tTime 0.088 (0.103)\tLoss 1.6560 (2.0330)\tPrec@1 53.125 (53.209)\tPrec@5 87.500 (77.618)\n",
      "Test: [37/143]\tTime 0.077 (0.102)\tLoss 1.8987 (2.0295)\tPrec@1 50.000 (53.125)\tPrec@5 84.375 (77.796)\n",
      "Test: [38/143]\tTime 0.076 (0.101)\tLoss 1.8192 (2.0241)\tPrec@1 59.375 (53.285)\tPrec@5 78.125 (77.804)\n",
      "Test: [39/143]\tTime 0.076 (0.101)\tLoss 2.6484 (2.0397)\tPrec@1 46.875 (53.125)\tPrec@5 65.625 (77.500)\n",
      "Test: [40/143]\tTime 0.077 (0.100)\tLoss 2.0055 (2.0388)\tPrec@1 50.000 (53.049)\tPrec@5 71.875 (77.363)\n",
      "Test: [41/143]\tTime 0.076 (0.099)\tLoss 2.2302 (2.0434)\tPrec@1 46.875 (52.902)\tPrec@5 71.875 (77.232)\n",
      "Test: [42/143]\tTime 0.090 (0.099)\tLoss 1.6656 (2.0346)\tPrec@1 65.625 (53.198)\tPrec@5 78.125 (77.253)\n",
      "Test: [43/143]\tTime 0.080 (0.099)\tLoss 2.4474 (2.0440)\tPrec@1 56.250 (53.267)\tPrec@5 75.000 (77.202)\n",
      "Test: [44/143]\tTime 0.089 (0.099)\tLoss 2.8959 (2.0629)\tPrec@1 43.750 (53.056)\tPrec@5 65.625 (76.944)\n",
      "Test: [45/143]\tTime 0.075 (0.098)\tLoss 1.5464 (2.0517)\tPrec@1 53.125 (53.057)\tPrec@5 90.625 (77.242)\n",
      "Test: [46/143]\tTime 0.087 (0.098)\tLoss 2.5411 (2.0621)\tPrec@1 40.625 (52.793)\tPrec@5 62.500 (76.928)\n",
      "Test: [47/143]\tTime 0.083 (0.097)\tLoss 2.6495 (2.0743)\tPrec@1 43.750 (52.604)\tPrec@5 68.750 (76.758)\n",
      "Test: [48/143]\tTime 0.075 (0.097)\tLoss 1.4722 (2.0621)\tPrec@1 62.500 (52.806)\tPrec@5 84.375 (76.913)\n",
      "Test: [49/143]\tTime 0.077 (0.097)\tLoss 2.1075 (2.0630)\tPrec@1 43.750 (52.625)\tPrec@5 81.250 (77.000)\n",
      "Test: [50/143]\tTime 0.077 (0.096)\tLoss 2.3916 (2.0694)\tPrec@1 43.750 (52.451)\tPrec@5 71.875 (76.900)\n",
      "Test: [51/143]\tTime 0.076 (0.096)\tLoss 2.1827 (2.0716)\tPrec@1 56.250 (52.524)\tPrec@5 78.125 (76.923)\n",
      "Test: [52/143]\tTime 0.077 (0.095)\tLoss 1.6052 (2.0628)\tPrec@1 53.125 (52.535)\tPrec@5 81.250 (77.005)\n",
      "Test: [53/143]\tTime 0.076 (0.095)\tLoss 1.6519 (2.0552)\tPrec@1 68.750 (52.836)\tPrec@5 81.250 (77.083)\n",
      "Test: [54/143]\tTime 0.076 (0.095)\tLoss 1.4288 (2.0438)\tPrec@1 62.500 (53.011)\tPrec@5 90.625 (77.330)\n",
      "Test: [55/143]\tTime 0.076 (0.094)\tLoss 1.7414 (2.0384)\tPrec@1 62.500 (53.181)\tPrec@5 81.250 (77.400)\n",
      "Test: [56/143]\tTime 0.077 (0.094)\tLoss 2.1881 (2.0410)\tPrec@1 56.250 (53.235)\tPrec@5 75.000 (77.357)\n",
      "Test: [57/143]\tTime 0.078 (0.094)\tLoss 2.4445 (2.0480)\tPrec@1 50.000 (53.179)\tPrec@5 71.875 (77.263)\n",
      "Test: [58/143]\tTime 0.076 (0.094)\tLoss 1.9651 (2.0466)\tPrec@1 50.000 (53.125)\tPrec@5 75.000 (77.225)\n",
      "Test: [59/143]\tTime 0.074 (0.093)\tLoss 2.5045 (2.0542)\tPrec@1 53.125 (53.125)\tPrec@5 75.000 (77.188)\n",
      "Test: [60/143]\tTime 0.088 (0.093)\tLoss 1.4058 (2.0436)\tPrec@1 71.875 (53.432)\tPrec@5 81.250 (77.254)\n",
      "Test: [61/143]\tTime 0.076 (0.093)\tLoss 2.1958 (2.0460)\tPrec@1 37.500 (53.175)\tPrec@5 81.250 (77.319)\n",
      "Test: [62/143]\tTime 0.090 (0.093)\tLoss 2.2829 (2.0498)\tPrec@1 31.250 (52.827)\tPrec@5 84.375 (77.431)\n",
      "Test: [63/143]\tTime 0.080 (0.093)\tLoss 1.8646 (2.0469)\tPrec@1 53.125 (52.832)\tPrec@5 71.875 (77.344)\n",
      "Test: [64/143]\tTime 0.076 (0.092)\tLoss 2.3131 (2.0510)\tPrec@1 50.000 (52.788)\tPrec@5 68.750 (77.212)\n",
      "Test: [65/143]\tTime 0.075 (0.092)\tLoss 2.2237 (2.0536)\tPrec@1 56.250 (52.841)\tPrec@5 78.125 (77.225)\n",
      "Test: [66/143]\tTime 0.086 (0.092)\tLoss 1.9764 (2.0525)\tPrec@1 46.875 (52.752)\tPrec@5 81.250 (77.285)\n",
      "Test: [67/143]\tTime 0.075 (0.092)\tLoss 2.4701 (2.0586)\tPrec@1 34.375 (52.482)\tPrec@5 78.125 (77.298)\n",
      "Test: [68/143]\tTime 0.075 (0.092)\tLoss 1.8110 (2.0550)\tPrec@1 46.875 (52.400)\tPrec@5 78.125 (77.310)\n",
      "Test: [69/143]\tTime 0.076 (0.091)\tLoss 1.7987 (2.0513)\tPrec@1 59.375 (52.500)\tPrec@5 84.375 (77.411)\n",
      "Test: [70/143]\tTime 0.085 (0.091)\tLoss 1.7075 (2.0465)\tPrec@1 62.500 (52.641)\tPrec@5 78.125 (77.421)\n",
      "Test: [71/143]\tTime 0.075 (0.091)\tLoss 2.7398 (2.0561)\tPrec@1 37.500 (52.431)\tPrec@5 68.750 (77.300)\n",
      "Test: [72/143]\tTime 0.090 (0.091)\tLoss 2.0133 (2.0555)\tPrec@1 59.375 (52.526)\tPrec@5 71.875 (77.226)\n",
      "Test: [73/143]\tTime 0.079 (0.091)\tLoss 2.4521 (2.0609)\tPrec@1 40.625 (52.365)\tPrec@5 75.000 (77.196)\n",
      "Test: [74/143]\tTime 0.075 (0.091)\tLoss 2.1948 (2.0627)\tPrec@1 53.125 (52.375)\tPrec@5 78.125 (77.208)\n",
      "Test: [75/143]\tTime 0.091 (0.091)\tLoss 1.9199 (2.0608)\tPrec@1 56.250 (52.426)\tPrec@5 81.250 (77.262)\n",
      "Test: [76/143]\tTime 0.079 (0.090)\tLoss 2.4765 (2.0662)\tPrec@1 53.125 (52.435)\tPrec@5 84.375 (77.354)\n",
      "Test: [77/143]\tTime 0.081 (0.090)\tLoss 1.3429 (2.0569)\tPrec@1 62.500 (52.564)\tPrec@5 84.375 (77.444)\n",
      "Test: [78/143]\tTime 0.075 (0.090)\tLoss 1.6670 (2.0520)\tPrec@1 65.625 (52.729)\tPrec@5 84.375 (77.532)\n",
      "Test: [79/143]\tTime 0.076 (0.090)\tLoss 1.7971 (2.0488)\tPrec@1 65.625 (52.891)\tPrec@5 84.375 (77.617)\n",
      "Test: [80/143]\tTime 0.074 (0.090)\tLoss 2.2430 (2.0512)\tPrec@1 56.250 (52.932)\tPrec@5 71.875 (77.546)\n",
      "Test: [81/143]\tTime 0.075 (0.090)\tLoss 1.5064 (2.0446)\tPrec@1 65.625 (53.087)\tPrec@5 84.375 (77.630)\n",
      "Test: [82/143]\tTime 0.075 (0.089)\tLoss 2.0148 (2.0442)\tPrec@1 40.625 (52.937)\tPrec@5 78.125 (77.636)\n",
      "Test: [83/143]\tTime 0.075 (0.089)\tLoss 2.1537 (2.0455)\tPrec@1 50.000 (52.902)\tPrec@5 75.000 (77.604)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: [84/143]\tTime 0.074 (0.089)\tLoss 2.3153 (2.0487)\tPrec@1 43.750 (52.794)\tPrec@5 78.125 (77.610)\n",
      "Test: [85/143]\tTime 0.075 (0.089)\tLoss 1.7025 (2.0447)\tPrec@1 50.000 (52.762)\tPrec@5 84.375 (77.689)\n",
      "Test: [86/143]\tTime 0.074 (0.089)\tLoss 2.4964 (2.0499)\tPrec@1 34.375 (52.550)\tPrec@5 78.125 (77.694)\n",
      "Test: [87/143]\tTime 0.074 (0.089)\tLoss 1.5056 (2.0437)\tPrec@1 62.500 (52.663)\tPrec@5 84.375 (77.770)\n",
      "Test: [88/143]\tTime 0.074 (0.088)\tLoss 2.8244 (2.0524)\tPrec@1 31.250 (52.423)\tPrec@5 56.250 (77.528)\n",
      "Test: [89/143]\tTime 0.074 (0.088)\tLoss 2.5306 (2.0578)\tPrec@1 50.000 (52.396)\tPrec@5 71.875 (77.465)\n",
      "Test: [90/143]\tTime 0.074 (0.088)\tLoss 2.2441 (2.0598)\tPrec@1 50.000 (52.370)\tPrec@5 68.750 (77.370)\n",
      "Test: [91/143]\tTime 0.075 (0.088)\tLoss 2.3531 (2.0630)\tPrec@1 43.750 (52.276)\tPrec@5 71.875 (77.310)\n",
      "Test: [92/143]\tTime 0.074 (0.088)\tLoss 2.8555 (2.0715)\tPrec@1 43.750 (52.184)\tPrec@5 65.625 (77.184)\n",
      "Test: [93/143]\tTime 0.074 (0.088)\tLoss 1.5988 (2.0665)\tPrec@1 65.625 (52.327)\tPrec@5 81.250 (77.227)\n",
      "Test: [94/143]\tTime 0.074 (0.088)\tLoss 2.3046 (2.0690)\tPrec@1 53.125 (52.336)\tPrec@5 71.875 (77.171)\n",
      "Test: [95/143]\tTime 0.074 (0.087)\tLoss 2.0302 (2.0686)\tPrec@1 59.375 (52.409)\tPrec@5 78.125 (77.181)\n",
      "Test: [96/143]\tTime 0.074 (0.087)\tLoss 1.9153 (2.0670)\tPrec@1 56.250 (52.448)\tPrec@5 81.250 (77.223)\n",
      "Test: [97/143]\tTime 0.076 (0.087)\tLoss 2.3775 (2.0702)\tPrec@1 46.875 (52.392)\tPrec@5 78.125 (77.232)\n",
      "Test: [98/143]\tTime 0.074 (0.087)\tLoss 2.8070 (2.0776)\tPrec@1 37.500 (52.241)\tPrec@5 59.375 (77.052)\n",
      "Test: [99/143]\tTime 0.074 (0.087)\tLoss 1.9964 (2.0768)\tPrec@1 53.125 (52.250)\tPrec@5 71.875 (77.000)\n",
      "Test: [100/143]\tTime 0.075 (0.087)\tLoss 2.2658 (2.0787)\tPrec@1 53.125 (52.259)\tPrec@5 71.875 (76.949)\n",
      "Test: [101/143]\tTime 0.076 (0.087)\tLoss 1.7152 (2.0751)\tPrec@1 65.625 (52.390)\tPrec@5 78.125 (76.961)\n",
      "Test: [102/143]\tTime 0.085 (0.087)\tLoss 2.3471 (2.0778)\tPrec@1 53.125 (52.397)\tPrec@5 71.875 (76.911)\n",
      "Test: [103/143]\tTime 0.091 (0.087)\tLoss 1.6839 (2.0740)\tPrec@1 59.375 (52.464)\tPrec@5 81.250 (76.953)\n",
      "Test: [104/143]\tTime 0.081 (0.087)\tLoss 2.3013 (2.0761)\tPrec@1 50.000 (52.440)\tPrec@5 75.000 (76.935)\n",
      "Test: [105/143]\tTime 0.083 (0.087)\tLoss 1.9951 (2.0754)\tPrec@1 56.250 (52.476)\tPrec@5 71.875 (76.887)\n",
      "Test: [106/143]\tTime 0.075 (0.086)\tLoss 1.4129 (2.0692)\tPrec@1 71.875 (52.658)\tPrec@5 84.375 (76.957)\n",
      "Test: [107/143]\tTime 0.076 (0.086)\tLoss 2.3595 (2.0719)\tPrec@1 46.875 (52.604)\tPrec@5 68.750 (76.881)\n",
      "Test: [108/143]\tTime 0.093 (0.086)\tLoss 1.5838 (2.0674)\tPrec@1 65.625 (52.724)\tPrec@5 90.625 (77.007)\n",
      "Test: [109/143]\tTime 0.081 (0.086)\tLoss 1.8837 (2.0657)\tPrec@1 46.875 (52.670)\tPrec@5 84.375 (77.074)\n",
      "Test: [110/143]\tTime 0.075 (0.086)\tLoss 2.7965 (2.0723)\tPrec@1 37.500 (52.534)\tPrec@5 65.625 (76.971)\n",
      "Test: [111/143]\tTime 0.093 (0.086)\tLoss 2.4132 (2.0753)\tPrec@1 46.875 (52.483)\tPrec@5 65.625 (76.869)\n",
      "Test: [112/143]\tTime 0.076 (0.086)\tLoss 2.5503 (2.0795)\tPrec@1 46.875 (52.434)\tPrec@5 75.000 (76.853)\n",
      "Test: [113/143]\tTime 0.075 (0.086)\tLoss 2.0328 (2.0791)\tPrec@1 56.250 (52.467)\tPrec@5 71.875 (76.809)\n",
      "Test: [114/143]\tTime 0.076 (0.086)\tLoss 2.2168 (2.0803)\tPrec@1 62.500 (52.554)\tPrec@5 71.875 (76.766)\n",
      "Test: [115/143]\tTime 0.086 (0.086)\tLoss 1.6965 (2.0770)\tPrec@1 53.125 (52.559)\tPrec@5 81.250 (76.805)\n",
      "Test: [116/143]\tTime 0.080 (0.086)\tLoss 1.4987 (2.0721)\tPrec@1 56.250 (52.591)\tPrec@5 84.375 (76.870)\n",
      "Test: [117/143]\tTime 0.088 (0.086)\tLoss 2.6416 (2.0769)\tPrec@1 40.625 (52.489)\tPrec@5 71.875 (76.827)\n",
      "Test: [118/143]\tTime 0.078 (0.086)\tLoss 2.3341 (2.0791)\tPrec@1 53.125 (52.495)\tPrec@5 75.000 (76.812)\n",
      "Test: [119/143]\tTime 0.075 (0.086)\tLoss 2.7118 (2.0843)\tPrec@1 37.500 (52.370)\tPrec@5 71.875 (76.771)\n",
      "Test: [120/143]\tTime 0.076 (0.086)\tLoss 2.9194 (2.0912)\tPrec@1 43.750 (52.299)\tPrec@5 65.625 (76.679)\n",
      "Test: [121/143]\tTime 0.083 (0.086)\tLoss 1.9418 (2.0900)\tPrec@1 46.875 (52.254)\tPrec@5 78.125 (76.691)\n",
      "Test: [122/143]\tTime 0.087 (0.086)\tLoss 2.2434 (2.0913)\tPrec@1 34.375 (52.109)\tPrec@5 78.125 (76.702)\n",
      "Test: [123/143]\tTime 0.076 (0.086)\tLoss 2.1209 (2.0915)\tPrec@1 50.000 (52.092)\tPrec@5 71.875 (76.663)\n",
      "Test: [124/143]\tTime 0.088 (0.086)\tLoss 2.5558 (2.0952)\tPrec@1 40.625 (52.000)\tPrec@5 71.875 (76.625)\n",
      "Test: [125/143]\tTime 0.084 (0.086)\tLoss 2.1271 (2.0955)\tPrec@1 59.375 (52.059)\tPrec@5 78.125 (76.637)\n",
      "Test: [126/143]\tTime 0.087 (0.086)\tLoss 1.8742 (2.0937)\tPrec@1 53.125 (52.067)\tPrec@5 78.125 (76.649)\n",
      "Test: [127/143]\tTime 0.078 (0.086)\tLoss 1.9321 (2.0925)\tPrec@1 56.250 (52.100)\tPrec@5 81.250 (76.685)\n",
      "Test: [128/143]\tTime 0.074 (0.086)\tLoss 2.2983 (2.0941)\tPrec@1 59.375 (52.156)\tPrec@5 78.125 (76.696)\n",
      "Test: [129/143]\tTime 0.088 (0.086)\tLoss 1.9905 (2.0933)\tPrec@1 62.500 (52.236)\tPrec@5 75.000 (76.683)\n",
      "Test: [130/143]\tTime 0.075 (0.085)\tLoss 2.0310 (2.0928)\tPrec@1 59.375 (52.290)\tPrec@5 78.125 (76.694)\n",
      "Test: [131/143]\tTime 0.074 (0.085)\tLoss 1.9338 (2.0916)\tPrec@1 56.250 (52.320)\tPrec@5 78.125 (76.705)\n",
      "Test: [132/143]\tTime 0.075 (0.085)\tLoss 1.8722 (2.0899)\tPrec@1 56.250 (52.350)\tPrec@5 78.125 (76.715)\n",
      "Test: [133/143]\tTime 0.075 (0.085)\tLoss 2.2237 (2.0909)\tPrec@1 40.625 (52.262)\tPrec@5 81.250 (76.749)\n",
      "Test: [134/143]\tTime 0.075 (0.085)\tLoss 2.5148 (2.0941)\tPrec@1 46.875 (52.222)\tPrec@5 71.875 (76.713)\n",
      "Test: [135/143]\tTime 0.078 (0.085)\tLoss 2.1365 (2.0944)\tPrec@1 46.875 (52.183)\tPrec@5 68.750 (76.654)\n",
      "Test: [136/143]\tTime 0.072 (0.085)\tLoss 2.3867 (2.0965)\tPrec@1 59.375 (52.235)\tPrec@5 71.875 (76.620)\n",
      "Test: [137/143]\tTime 0.071 (0.085)\tLoss 2.7779 (2.1015)\tPrec@1 40.625 (52.151)\tPrec@5 68.750 (76.562)\n",
      "Test: [138/143]\tTime 0.075 (0.085)\tLoss 1.9832 (2.1006)\tPrec@1 43.750 (52.091)\tPrec@5 90.625 (76.664)\n",
      "Test: [139/143]\tTime 0.072 (0.085)\tLoss 2.1252 (2.1008)\tPrec@1 50.000 (52.076)\tPrec@5 71.875 (76.629)\n",
      "Test: [140/143]\tTime 0.071 (0.085)\tLoss 2.2792 (2.1020)\tPrec@1 53.125 (52.083)\tPrec@5 81.250 (76.662)\n",
      "Test: [141/143]\tTime 0.075 (0.085)\tLoss 2.2065 (2.1028)\tPrec@1 50.000 (52.069)\tPrec@5 78.125 (76.673)\n",
      "Test: [142/143]\tTime 0.060 (0.084)\tLoss 2.6173 (2.1054)\tPrec@1 43.478 (52.025)\tPrec@5 69.565 (76.637)\n",
      " * Prec@1 52.025 Prec@5 76.637\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "52.02539960169182"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(loaders['test'],vgg16,criterion,color=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
